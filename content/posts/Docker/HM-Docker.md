---
title: "Docker"
date: 2024-03-21T08:36:54Z
draft: true
tags: ["Docker"]
---

Docker image
Docker container
Docker registry

## 1. Introduction

- CNCF is cloud native computing foundation
- Container image: a universal app packager
- Cross platform
- Docker file
- OCI is an image standard
- Stacking up the layers
- Image registry & app distribution (Docker hub)
- Sha hash is generated by the image we build

---
- docker container
- docker engine
一层虚拟化层，如果不开这个engine，wsl中就无法使用docker
```bash
root@IVT-WKS-000223:~# docker

The command 'docker' could not be found in this WSL 2 distro.
We recommend to activate the WSL integration in Docker Desktop settings.

For details about using Docker Desktop with WSL 2, visit:

https://docs.docker.com/go/wsl2/
```

`docker run my-python-app`会允许在docker container中执行某些命令，这类似于namespace，让程序看不见其他的系统环境，会给进程提供自己的文件系统，process list，以及网络，like its own system
你可以多次执行相同的`docker run`，

those two process are isolated from each other

## 4 Play the containers as a boss
### 4.1 容器的启动和关闭
? what is TCP/TLS
docker client 和 docker engine之间的沟通可以通过各种传输协议来进行通讯

engine版本的查看
`docker version`
`docker info`                               more details

image的创建
`docker build`

image镜像的拉取和上传
`docker push` the image on the image registry
`docker pull` the image from the image registry

容器监控
`docker ps`                                 展示正在运行的容器
`docker container ls -a`                    展示所有容器（包括被stop的）
`docker container logs <container_name>`    展示日志
`docker container top <container_name>`     展示运行的进程

`docker container inspect <container_name>`
`docker container stats`

容器创建（启动）/删除/
`docker run -d -p <host_port>:<container_port> <container_name>`    `-d` means detach   -p publish
`docker run`的时候，先会下载镜像，然后创建容器，将镜像中的内容拷贝到容器里，最后再启动进程
`docker start`                              开启容器
`docker container stop <container_ID>`
`docker container rm -f <container_name>`   强制删除（就算在运行也会删除）

#### 进程监控issue
关于宿主机使用`ps aux`指令无法观察容器进程的问题
![alt text](asset/image.png)
![alt text](<asset/image (1).png>)

Getting a shell inside a container 你不再需要使用ssh去切进容器，宿主机对容器有绝对的控制权
`docker run -it`                            -t sudo-tty

让nginx的启动command变成bash
```bash
root@IVT-WKS-000223:~# docker container run -it --name proxy nginx bash
root@4d27bff1d658:/#
root@4d27bff1d658:/#
root@4d27bff1d658:/# ps
bash: ps: command not found
```

`docker container start -ai <container_name>`                   在关闭容器后重启容器，并进入该容器的cmd
`docker container exec -it <container_name> <command(bash)>`    在运行的容器中开一个bash进程
`docker container run --rm -it ...`                             在容器关闭后删除该容器

不知道是什么原因错咯，无法复现
![alt text](asset/2024-03-15_16-19.png)

### 4.2 网络管理
Best practice is to create a new virtual network for each app.
![alt text](asset/2024-03-15_17-12.png)
`docker run -d -p <host_port>:<container_port> <container_name>`    `-d` means detach   -p publish
`docker container port <container>`                             查看容器端口
`docker container inspect --format '{{ .NetworkSettings.IPAddress }}' <container_name>`     查看容器的ip地址
容器的ip地址和宿主机的ip地址是不一样的

Microservices cannot rely on IP addresses, ...
Too dynamic and too flexible, ...
Docker 在微服务的场景下一般使用container names来进行通讯，而不是IP地址

`docker network ls`                     
`docker network insepct <network_name>`             查看某一private network中有的container
`docker network create --driver`        创建网络
`docker network connect/disconnect`     类似于将网卡插入container

### 4.3 解析Docker子网
```bash
root@IVT-WKS-000223:/mnt/c/Users/hzhang3/Documents/HouzeZHANG.github.io/content/posts/Docker# docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
a97a74aca4ff   bridge    bridge    local
9c8654069879   host      host      local
efd7f1376d1b   none      null      local
```
- `bridge` network is the default private(virtual) network created by the docker.
- `host` network用于跳过Virtual network，让容器直接和物理interface进行通信，在某些情况下能够降低throughput时延，only in certain situations...
- `none` network意味着有这么一个网络，但是什么也没有接。

```bash
root@IVT-WKS-000223:/mnt/c/Users/hzhang3/Documents/HouzeZHANG.github.io/content/posts/Docker# docker network inspect bridge
[
    {
        "Name": "bridge",
        "Id": "a97a74aca4ff2f7d0e4b154c84a7974001449a7ba41899800dfd0e40bf5083fe",
        "Created": "2024-03-18T09:00:33.692926496Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "4d27bff1d6585a4df180e9353100dfeecd8400ec8c5c206e6e8980125d42e0a1": {
                "Name": "proxy",
                "EndpointID": "0eb7658914853f0431c184e7b22aac887a63b72bade35d6a51cf0f9e8fb3c65f",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            }
        },
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {}
    }
]
```

观察其中的`IPAM`项，这是所有容器默认的子网地址，以及默认网关，这个网关用于和实际的physical network进行通讯
```bash
"IPAM": {
    "Driver": "default",
    "Options": null,
    "Config": [
        {
            "Subnet": "172.17.0.0/16",
            "Gateway": "172.17.0.1"
        }
    ]
},
```

自己创建的新子网会默认使用bridge driver，总之driver是一个extension，是一个built-in或3rd party extension that gives you virtual network features
```bash
root@IVT-WKS-000223:/mnt/c/Users/hzhang3/Documents/HouzeZHANG.github.io/content/posts/Docker# docker network create my_app_net
306419ab57360f6f6f671741254b2c7b7d8e7c257c281bcc7bc9d81cacb8e6d3
root@IVT-WKS-000223:/mnt/c/Users/hzhang3/Documents/HouzeZHANG.github.io/content/posts/Docker# docker network ls
NETWORK ID     NAME         DRIVER    SCOPE
a97a74aca4ff   bridge       bridge    local
9c8654069879   host         host      local
306419ab5736   my_app_net   bridge    local
efd7f1376d1b   none         null      local
```

使用`--network`来在某个`subnet`中启动container
```bash
root@IVT-WKS-000223:/mnt/c/Users/hzhang3/Documents/HouzeZHANG.github.io/content/posts/Docker# docker run -d --name subnet_nginx --network my_app_net nginx
c13a94d07a5777cc359f2250f7fab36417d20cd9328ab338718c194756872c63
root@IVT-WKS-000223:/mnt/c/Users/hzhang3/Documents/HouzeZHANG.github.io/content/posts/Docker# docker network inspect my_app_net
[
    {
        "Name": "my_app_net",
        "Id": "306419ab57360f6f6f671741254b2c7b7d8e7c257c281bcc7bc9d81cacb8e6d3",
        "Created": "2024-03-18T09:43:33.052883345Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "172.18.0.0/16",
                    "Gateway": "172.18.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "c13a94d07a5777cc359f2250f7fab36417d20cd9328ab338718c194756872c63": {
                "Name": "subnet_nginx",
                "EndpointID": "258ad79511c43d7117b70d21277b205962a51304c44826cd1ed3e43cfcda9409",
                "MacAddress": "02:42:ac:12:00:02",
                "IPv4Address": "172.18.0.2/16",
                "IPv6Address": ""
            }
        },
        "Options": {},
        "Labels": {}
    }
]
```

一个container可以连接多个subnet，将某个container连接到某个subnet，在使用`docker network connect`之后，查看目标容器会发现多了一个network，且这俩端口的ip地址是不同的，新的subnet的网络地址是由DHCP分配的
```bash
root@IVT-WKS-000223:/mnt/c/Users/hzhang3/Documents/HouzeZHANG.github.io/content/posts/Docker# docker container inspect 4d27bff1d6585a4df180e9353100dfeecd8400ec8c5c206e6e8980125d42e0a1
...
        "NetworkSettings": {
            "Bridge": "",
            "SandboxID": "1e7e23b5f3e927f7de0d07615cb8506b1848fdb5ae33ade6e03f6cb40706217c",
            "SandboxKey": "/var/run/docker/netns/1e7e23b5f3e9",
            "Ports": {
                "80/tcp": null
            },
            "HairpinMode": false,
            "LinkLocalIPv6Address": "",
            "LinkLocalIPv6PrefixLen": 0,
            "SecondaryIPAddresses": null,
            "SecondaryIPv6Addresses": null,
            "EndpointID": "0eb7658914853f0431c184e7b22aac887a63b72bade35d6a51cf0f9e8fb3c65f",
            "Gateway": "172.17.0.1",
            "GlobalIPv6Address": "",
            "GlobalIPv6PrefixLen": 0,
            "IPAddress": "172.17.0.2",
            "IPPrefixLen": 16,
            "IPv6Gateway": "",
            "MacAddress": "02:42:ac:11:00:02",
            "Networks": {
                "bridge": {
                    "IPAMConfig": null,
                    "Links": null,
                    "Aliases": null,
                    "MacAddress": "02:42:ac:11:00:02",
                    "NetworkID": "a97a74aca4ff2f7d0e4b154c84a7974001449a7ba41899800dfd0e40bf5083fe",
                    "EndpointID": "0eb7658914853f0431c184e7b22aac887a63b72bade35d6a51cf0f9e8fb3c65f",
                    "Gateway": "172.17.0.1",
                    "IPAddress": "172.17.0.2",
                    "IPPrefixLen": 16,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "DriverOpts": null,
                    "DNSNames": null
                },
                "my_app_net": {
                    "IPAMConfig": {},
                    "Links": null,
                    "Aliases": [
                        "4d27bff1d658"
                    ],
                    "MacAddress": "02:42:ac:12:00:03",
                    "NetworkID": "306419ab57360f6f6f671741254b2c7b7d8e7c257c281bcc7bc9d81cacb8e6d3",
                    "EndpointID": "0f1ad05d4f93fabc56b71f0477033e06c55358701eee8c6eaa1ee06a3de55463",
                    "Gateway": "172.18.0.1",
                    "IPAddress": "172.18.0.3",
                    "IPPrefixLen": 16,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "DriverOpts": {},
                    "DNSNames": [
                        "proxy",
                        "4d27bff1d658"
                    ]
                }
            }
        }
...
```

现在就不难明白为什么在`docker run`的时候我们需要手动`-p`了，因为docker希望保护容器，不让其暴露在公有网络中

### 4.4 DNS Naming

使用DNS而不是依赖IP的好处是IP会变化，但是DNS域名一般不会变化，在公司的数据库系统中也发现了类似的设计，每次重新deploy后，数据库的域名不变，但是IP会变，使用数据库管理软件对数据库进行连接的时候通过AWS的内网域名进行连接而不是IP地址，会简化操作

IP什么时候会变化？如果在cluster中启动节点的顺序变化了，DHCP分配的IP地址就可能会变

Docker daemon has a built-in DNS server that containers use by default

测试某一网络中DNS是否起作用
```bash
docker container exec -it my_nginx ping new_nginx
```

`docker create --link`
Docker compose will make all these easier

#### DNS Round Robin Test

Requirement: Docker engine >= 1.11
Round Robin is not a true load balancer!

`--network-alias search`参数将容器的DNS别名设置为`search`

BP: `docker container run -d --net dude --net-alias search elasticsearch:2`

`docker container run --rm --net dude alpine nslookup search`
这行命令在子网中创建一个最小linux容器，然后执行`nslookup`进行DNS查询

`docker container run --rm --net dude centos curl -s 9200`
创建一个临时的centos的容器然后执行`curl`

```bash
houze@IVT-WKS-000223:~$ docker run -d --network my_search_network --network-alias search elasticsearch:2
fc89374f71661b329870084678bf2291c624920d5e5b4a838350997af918d6bd
houze@IVT-WKS-000223:~$ docker run -d --network my_search_network --network-alias search elasticsearch:2
02f2c4b8d74b39c43763b279c5c7209a07e6d74f3679c0d61b9c407c08184de1
houze@IVT-WKS-000223:~$ docker ps -a
CONTAINER ID   IMAGE             COMMAND                  CREATED             STATUS                      PORTS                NAMES
02f2c4b8d74b   elasticsearch:2   "/docker-entrypoint.…"   4 seconds ago       Up 3 seconds                9200/tcp, 9300/tcp   sweet_wescoff
fc89374f7166   elasticsearch:2   "/docker-entrypoint.…"   6 seconds ago       Up 5 seconds                9200/tcp, 9300/tcp   wonderful_lichterman
```
在把ubuntu加入子网后，我们可以观察到如下现象
```bash
root@01432d9b9db7:/# curl -s search:9200
{
  "name" : "Lockheed",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "YmGvtgxnSr6wdD6SEW7JOw",
  "version" : {
    "number" : "2.4.6",
    "build_hash" : "5376dca9f70f3abef96a77f4bb22720ace8240fd",
    "build_timestamp" : "2017-07-18T12:17:44Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.4"
  },
  "tagline" : "You Know, for Search"
}
root@01432d9b9db7:/# curl -s search:9200
{
  "name" : "Hanna Levy",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "iNEuDoCCRpyxUv3hDrs8uw",
  "version" : {
    "number" : "2.4.6",
    "build_hash" : "5376dca9f70f3abef96a77f4bb22720ace8240fd",
    "build_timestamp" : "2017-07-18T12:17:44Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.4"
  },
  "tagline" : "You Know, for Search"
}
```

## Section 5 Container Images

### 5.1 Docker Images and Docker Hub

What an image include?
- app binaries and dependencies
- metadata about the image data and how to run the image

Points:
- The host provides the kernel, so there is no kernel in the image(different from the VM), GO's executable file is very small.
- Docker hub is similar to the apt package system for containers.

---

`docker image ls`           查看所有docker镜像
`docker image history`      history of the image layer

*Stack of image layers*: image是分层存储的，一系列image类似于一棵版本树。靠近根节点的节点不会多次存储

*Read write layer*: 如果我们创建一个container，我们就在image层上添加一层*read write layer*，container本质上并不是启动了的image，`docker ps`是显示在运行的*container*，而`docker ps -a`是显示所有*container*

*containers*类似于file system，*docker storage driver*负责在*stack of image layer*上堆，*image*的存储，传播以及`docker build`是很节约时间空间的，用的是*copy on write*技术

这些*missing layer*是内部层，正是这些内部层的堆叠构成了`ca2b0f26964c`这个容器

```bash
houze@IVT-WKS-000223:~$ docker image history ubuntu
IMAGE          CREATED       CREATED BY                                      SIZE      COMMENT
ca2b0f26964c   2 weeks ago   /bin/sh -c #(nop)  CMD ["/bin/bash"]            0B
<missing>      2 weeks ago   /bin/sh -c #(nop) ADD file:21c2e8d95909bec6f…   77.9MB
<missing>      2 weeks ago   /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B
<missing>      2 weeks ago   /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B
<missing>      2 weeks ago   /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B
<missing>      2 weeks ago   /bin/sh -c #(nop)  ARG RELEASE                  0B
```

`docker image inspect <image_name>`用于查看镜像的*metadata*，比如你可以查看到`nginx`镜像的默认监听端口，可以查看到镜像运行的*architecture*和OS

---

*image tag*: indicates branch + version, a pointer pointing to the specific image

*image id*和*image tag*是一对多的关系

unique organizational based image like `docker pull mysql/mysql-server`

如果用两个不同的，指向同一个*image*的*tag*拉两次*image*，在`docker image ls`里能看到他们的哈希值相同
```bash
houze@IVT-WKS-000223:~$ docker image ls
REPOSITORY      TAG       IMAGE ID       CREATED        SIZE
mongo           latest    79112eff9c89   2 weeks ago    756MB
ubuntu          latest    ca2b0f26964c   2 weeks ago    77.9MB
postgres        16        b9390dd1ea18   3 weeks ago    431MB
postgres        latest    b9390dd1ea18   3 weeks ago    431MB
```

使用`docker image tag <source_image> <target_image>`来*retage*

*latest*一般意味着*default image*

如果你想将一个docker image checkout，你可以先retag它，然后docker push到你自己的docker hub账户下

`docker login <server_name>`        用于登陆docker服务器
在一台不信任的设备上完成操作后，记得`docker logout`

### 5.2 Docker File

- *docker file*: A recipe for creating your image
- 每一个*docker file*中的操作都是一层*image layer*
- 所以如果你不想创建太多*layers*，你可以用`&&`将多条shell语句封装成一个*layer*
- 在容器中做日志的最方便的方法是将日志内容打到`/dev/stdout`和`/dev/stderr`，docker给容器们提供了相应的日志功能

`docker build -f <specific_docker_file>`

1. `FROM`: 从debian或者ubuntu进行构建的原因是可以很方便的使用这些操作系统的PM（package manager）
2. `ENV`: 在container中设置环境变量的方法
3. `EXPOSE`: 打开端口。docker默认不开任何端口，`EXPOSE`完毕后，在host中也需要使用`-p`选项进行端口映射
4. `CMD`: run when container is launched

---

`docker build -t <tag_name_assigned> .`

Things change less should be put on the top of the docker file, change more should be put on the tail of the docker file

---

`WORKDIR <path_to_move_to>`切换工作目录的最佳实践

`COPY <host_file> <image_file>`将宿主机文件拷贝到容器中
BP: 如果只想单次修改，建议使用`COPY`，如果希望持续同步，建议使用*bind mounting*

不一定总是需要`CMD`指令，因为它被包括在`FROM`字句中了

---

`docker image prune`
`docker system prune`
`docker image prune -a`
`docker system df`

What is the VM *auto-shrink*?

```Dockerfile
# NOTE: this example is taken from the default Dockerfile for the official nginx Docker Hub Repo
# https://hub.docker.com/_/nginx/
# NOTE: This file is slightly different than the video, because nginx versions have been updated 
#       to match the latest standards from docker hub... but it's doing the same thing as the video
#       describes

FROM debian:bookworm-slim
# all images must have a FROM
# usually from a minimal Linux distribution like debian or (even better) alpine
# if you truly want to start with an empty container, use FROM scratch

LABEL maintainer="NGINX Docker Maintainers <docker-maint@nginx.com>"

# optional environment variable that's used in later lines and set as envvar when container is running
ENV NGINX_VERSION   1.25.3
ENV NJS_VERSION     0.8.2
ENV PKG_RELEASE     1~bookworm


RUN set -x \
# create nginx user/group first, to be consistent throughout docker variants
    && groupadd --system --gid 101 nginx \
    && useradd --system --gid nginx --no-create-home --home /nonexistent --comment "nginx user" --shell /bin/false --uid 101 nginx \
    && apt-get update \
    && apt-get install --no-install-recommends --no-install-suggests -y gnupg1 ca-certificates \
    && \
    NGINX_GPGKEY=573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62; \
    NGINX_GPGKEY_PATH=/usr/share/keyrings/nginx-archive-keyring.gpg; \
    export GNUPGHOME="$(mktemp -d)"; \
    found=''; \
    for server in \
        hkp://keyserver.ubuntu.com:80 \
        pgp.mit.edu \
    ; do \
        echo "Fetching GPG key $NGINX_GPGKEY from $server"; \
        gpg1 --keyserver "$server" --keyserver-options timeout=10 --recv-keys "$NGINX_GPGKEY" && found=yes && break; \
    done; \
    test -z "$found" && echo >&2 "error: failed to fetch GPG key $NGINX_GPGKEY" && exit 1; \
    gpg1 --export "$NGINX_GPGKEY" > "$NGINX_GPGKEY_PATH" ; \
    rm -rf "$GNUPGHOME"; \
    apt-get remove --purge --auto-remove -y gnupg1 && rm -rf /var/lib/apt/lists/* \
    && dpkgArch="$(dpkg --print-architecture)" \
    && nginxPackages=" \
        nginx=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-xslt=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-geoip=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-image-filter=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-njs=${NGINX_VERSION}+${NJS_VERSION}-${PKG_RELEASE} \
    " \
    && case "$dpkgArch" in \
        amd64|arm64) \
# arches officialy built by upstream
            echo "deb [signed-by=$NGINX_GPGKEY_PATH] https://nginx.org/packages/mainline/debian/ bookworm nginx" >> /etc/apt/sources.list.d/nginx.list \
            && apt-get update \
            ;; \
        *) \
# we're on an architecture upstream doesn't officially build for
# let's build binaries from the published source packages
            echo "deb-src [signed-by=$NGINX_GPGKEY_PATH] https://nginx.org/packages/mainline/debian/ bookworm nginx" >> /etc/apt/sources.list.d/nginx.list \
            \
# new directory for storing sources and .deb files
            && tempDir="$(mktemp -d)" \
            && chmod 777 "$tempDir" \
# (777 to ensure APT's "_apt" user can access it too)
            \
# save list of currently-installed packages so build dependencies can be cleanly removed later
            && savedAptMark="$(apt-mark showmanual)" \
            \
# build .deb files from upstream's source packages (which are verified by apt-get)
            && apt-get update \
            && apt-get build-dep -y $nginxPackages \
            && ( \
                cd "$tempDir" \
                && DEB_BUILD_OPTIONS="nocheck parallel=$(nproc)" \
                    apt-get source --compile $nginxPackages \
            ) \
# we don't remove APT lists here because they get re-downloaded and removed later
            \
# reset apt-mark's "manual" list so that "purge --auto-remove" will remove all build dependencies
# (which is done after we install the built packages so we don't have to redownload any overlapping dependencies)
            && apt-mark showmanual | xargs apt-mark auto > /dev/null \
            && { [ -z "$savedAptMark" ] || apt-mark manual $savedAptMark; } \
            \
# create a temporary local APT repo to install from (so that dependency resolution can be handled by APT, as it should be)
            && ls -lAFh "$tempDir" \
            && ( cd "$tempDir" && dpkg-scanpackages . > Packages ) \
            && grep '^Package: ' "$tempDir/Packages" \
            && echo "deb [ trusted=yes ] file://$tempDir ./" > /etc/apt/sources.list.d/temp.list \
# work around the following APT issue by using "Acquire::GzipIndexes=false" (overriding "/etc/apt/apt.conf.d/docker-gzip-indexes")
#   Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied)
#   ...
#   E: Failed to fetch store:/var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages  Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied)
            && apt-get -o Acquire::GzipIndexes=false update \
            ;; \
    esac \
    \
    && apt-get install --no-install-recommends --no-install-suggests -y \
                        $nginxPackages \
                        gettext-base \
                        curl \
    && apt-get remove --purge --auto-remove -y && rm -rf /var/lib/apt/lists/* /etc/apt/sources.list.d/nginx.list \
    \
# if we have leftovers from building, let's purge them (including extra, unnecessary build deps)
    && if [ -n "$tempDir" ]; then \
        apt-get purge -y --auto-remove \
        && rm -rf "$tempDir" /etc/apt/sources.list.d/temp.list; \
    fi \
# forward request and error logs to docker log collector
    && ln -sf /dev/stdout /var/log/nginx/access.log \
    && ln -sf /dev/stderr /var/log/nginx/error.log \
# create a docker-entrypoint.d directory
    && mkdir /docker-entrypoint.d

# COPY docker-entrypoint.sh /
# COPY 10-listen-on-ipv6-by-default.sh /docker-entrypoint.d
# COPY 20-envsubst-on-templates.sh /docker-entrypoint.d
# COPY 30-tune-worker-processes.sh /docker-entrypoint.d
# ENTRYPOINT ["/docker-entrypoint.sh"]

EXPOSE 80
# expose these ports on the docker virtual network
# you still need to use -p or -P to open/forward these ports on host

STOPSIGNAL SIGQUIT

CMD ["nginx", "-g", "daemon off;"]
# required: run this command when container is launched
# only one CMD allowed, so if there are multiple, last one wins

```

## Section 6 Persistent Data: Volumes

*immutable infrastructure*: only re-deploy containers, never change it
*ephemeral designed*
*separation of concerns*: separate the unique data from container(like db)

*UFS*: union file system

Containers are persistent by default... Only remove them, their UFS layer will go away.

*Auto-scaling application*'s solution:

- *Volumes*: make special location outside of container UFS, 允许用户在移除容器的时候保留其上的unique数据，然后将其attach到其他容器上，对于container来说这是一个local file path
- *Bind mount*: link container path to host path, 这个挂载对于container来说是transparent的

### 6.1 Volumes

```dockerfile
# mysql
VOLUME /var/lib/mysql
```

Volumes need manual deletion, you cannot clean them up just remove the container.
这个volume操作的结果可以通过`docker image inspect mysql`来查看

`docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql`

<?>
`docker volume ls`                              查看所有的*卷*
`docker volume inspect <volume_sha>`            查看容器希望使用的的*卷*位置

Windows或者mac设备上做*volume*，这些数据会被放在虚拟机中，对于宿主机来说无法直接访问，这个限制可以通过mount volume来绕过这个限制

有一个比较麻烦的事情是，*non-named-volume*对于容器来讲，是容易区分的，但是仅仅通过*non-named-volume*，我们无法识别该*volume*和哪个容器相关

---

*Named-volume*: `docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v [<name_of_volume>:]<path> mysql`

BP: 用项目名字给数据库的volume进行命名

<?>
`docker volume create`

---

path expansion:
- `${pwd}`  powershell
- `%cd%`    cmd
- `$(pwd)`  linux/macOS bash

`docker container run -d --name nginx -p 80:80 -v $(pwd):/usr/share/nginx/html nginx`

### 6.2 Bind Mounting

- Having two locations pointing to the same location on the disk.
- Doesn't require a volume to work.
- Mapping a host file or directory to a container file or directory.
- Skip UFS.
- Bind mounting is host-specific so you cannot configure it in the *Dockerfile*.

`... run -v /Users/bret/stuff:/path/container`          mac/linux
`... run -v //Users/bret/stuff:/path/container`         windows

挂载完毕后切入container检查效果
`docker container exec -it nginx bash`

Bind Mounting is used for local development and local testing.

Before making a new volume in a docker run -v command for a MySQL container, where could you look to see where MySQL expects the data path to be?
The answer is Docker Hub.

### 6.3 Database Passwords in Containers

We all know databases usually need passwords, but since the dawn of Docker, the postgres image (and a few others like redis) has allowed you to do a simple docker run on it and it starts without a password. Sure you could set a password but it didn't require one.

In Feburary 2020 that changed, and will affect using postgres in this course (and my others). When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change).

For docker run, and the forthcoming Docker Compose sections, you need to either set a password with the environment variable:

`POSTGRES_PASSWORD=mypasswd`

Or tell it to ignore passwords with the environment variable:

`POSTGRES_HOST_AUTH_METHOD=trust`

Note this change was in the Docker Hub image, and not a change in postgres itself.

---

### 6.4 Version Best Practice

Also note if I or you were pinning versions, as we should, this wouldn't have surprised us. I tend to only pin to the minor version in this course (9.6) rather than the *patch version* (9.6.16) to keep you a bit more secure in the course. In the real world, **I always pin my production apps to the patch version. It's the only safe way to operate**. By pinning to the minor version in the courses, I prevent any major changes from breaking the course (in theory ha ha), yet also ensure you're running the latest patches (which would fix any bugs or security problems). In this, *very rare case*, the maintainer of the official postgres decided to introduce a breaking change in the *image* to a patch release of the app. <?>The two aren't related, and it kinda shows off a weakness of the Docker Hub model... that there is no version of the Docker Hub image really, it's just tracking the upstream postgres versions... so then if any Docker Hub change would break something, it can't easily be tracked as a separate version from the app itself. Oh well, just remember to always pin the whole image version for things you care about.

I've updated the course repo files to indicate this change, but if you've cloned the repo before February 18th, 2020, you will need to update or replace your clone.

### 6.5 Database upgrade: real-world scenario

Major upgrade may require schema changes for the db.

```bash
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/ans-db-1# docker container run -d --name pg1 -e POSTGRES_HOST_AUTH_METHOD=trust -v psql-data:/var/lib/postgresql/data postgres:9.6.1
794bb1dba69249bf56cf7146968ea4f854df2bf858f24d36a6471c5d3cdd2cec
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/ans-db-1# docker ps
CONTAINER ID   IMAGE            COMMAND                  CREATED         STATUS         PORTS      NAMES
794bb1dba692   postgres:9.6.1   "/docker-entrypoint.…"   3 seconds ago   Up 2 seconds   5432/tcp   pg1
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/ans-db-1#  docker volume ls
DRIVER    VOLUME NAME
local     1b8edb5bebaade8c9208b219c497fc7b0719bc1754a1a60b066f57621ca78c54
local     2c970b0cb77432298d951f75538e0abbacc621633bb17860f8c286b05d42e5ee
local     9ac01f466044cfbc49d72cdebf6f1ecd14a8886bcb8f431fb028d0b85b05b5fd
local     9eab4e05930a0fe4e1995f6658f7d607a414870ddf4e6e5578563316b1444612
local     13b0ba6845faa610f5eed7f8dd8144d1d2befb0eaa9ce94667ab4f9ec47dd0b5
local     71bc7c6b2ee7a4b796a1e73349c62e6ef9eaec5b8966e071fcfc9a1c7d92cbbc
local     83ac79f371244f2f63da741b054fa6fdf18f71dca74a8bbfe1283422ff1d1843
local     87cc86016a3d89e16372363b0bf5b5d5dbddb339918999e13fd06c667e83a93e
local     506e581040675eb2a39835b532d4601067506ad22f7453f4b038fe156a02dad9
local     4754bdd64e34596a470936b1a3513292b23f7b9af8c19b1eec8795013a3e8322
local     8635e8650f912205128d12f102ee804fca2001fd39e3ff8faf1f08b19f25ed54
local     a37ca698cf7dd3bd7226c33a1434298bd17c450972cee93df7b4e18484ba4dd3
local     e1ea02aa457a8ffff3ac844624f8cbc4e94fef0a44bdd8a5103dac83f1de2128
local     e9b91a86901fad7ec287ccec9cc5a3e1b588cd0fc931c7b7e36319a1f7c74c9f
local     psql-data
```

在没有*volume*的情况下pg做的初始化日志

```bash
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/ans-db-1# docker container logs 794bb1dba692
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "en_US.utf8".
The default database encoding has accordingly been set to "UTF8".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/postgresql/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting dynamic shared memory implementation ... posix
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok

WARNING: enabling "trust" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.
syncing data to disk ... ok

Success. You can now start the database server using:

    pg_ctl -D /var/lib/postgresql/data -l logfile start

****************************************************
WARNING: No password has been set for the database.
         This will allow anyone with access to the
         Postgres port to access your database. In
         Docker's default configuration, this is
         effectively any other container on the same
         system.

         Use "-e POSTGRES_PASSWORD=password" to set
         it in "docker run".
****************************************************
waiting for server to start....LOG:  could not bind IPv6 socket: Cannot assign requested address
HINT:  Is another postmaster already running on port 5432? If not, wait a few seconds and retry.
LOG:  database system was shut down at 2024-03-19 10:37:58 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  autovacuum launcher started
LOG:  database system is ready to accept connections
 done
server started
ALTER ROLE


/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*

LOG:  received fast shutdown request
waiting for server to shut down....LOG:  aborting any active transactions
LOG:  autovacuum launcher shutting down
LOG:  shutting down
LOG:  database system is shut down
 done
server stopped

PostgreSQL init process complete; ready for start up.

LOG:  database system was shut down at 2024-03-19 10:38:00 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started
```

shorter log printed by reusing volume

```bash
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/ans-db-1# docker container run -d --name pg2 -e POSTGRES_HOST_AUTH_METHOD=trust -v psql-data:/var/lib/postgresql/data postgres:9.6.2
58a5e9e9c978268ccdd613af8ff225aaa6ebded38f69ddca4c9e21fa8bcde2ab
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/ans-db-1# docker container logs
 pg2
LOG:  database system was shut down at 2024-03-19 10:40:52 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/ans-db-1#
```

### 6.6 <?> File Permissions Across Multiple Containers

At some point you'll have file permissions problems with container apps not having the permissions they need. Maybe you want multiple containers to access the same volume(s). Or maybe you're bind-mounting existing files into a container.

Note that the below info is about pure Linux hosts, like production server setups. If you're using Docker Desktop locally, it will translate permissions from your host (macOS & Windows) into the container (Linux) automatically, but when working on pure Linux servers with just dockerd, no translation is made.

#### How file permissions work across multiple containers accessing the same volume or bind-mount

File ownership between containers and the host are just **numbers**. They stay consistent no matter how you run them. Sometimes you see friendly user names in commands like ls but those are just name-to-number aliases that you'll see in `/etc/passwd` and `/etc/group`. Your host has those files, and usually, your containers will have their own. They are usually different. These files are really just for humans to see friendly names. **The Linux Kernel only cares about IDs**, which are attached to each file and directory in the file system itself, and those IDs are the same no matter which process accesses them.

When a container is just accessing its own files, this isn't usually an issue.

But for multiple containers accessing the same volume or bind-mount, problems can arise in two ways:

1. Problem one: The `/etc/passwd` is different across containers. Creating a named user in one container and running as that user may use ID 700, but that same name in another container with a different `/etc/passwd` may use a different ID for that same username. That's why I only care about IDs when trying to sync up permissions. You'll see this confusion if you're running a container on a Linux VM and it had a volume or bind-mount. If you do an ls on those files from the host, it may show them owned by ubuntu or node or systemd, etc. Then if you run ls inside the container, it may show a different friendly username. The IDs are the same in both cases, but the host will have a different passwd file than the container, and show you different friendly names. Different names are fine, because it's only ID that counts. Two processes trying to access the same file must have a matching user ID or group ID.

2. Problem two: Your two containers are running as different users. Maybe the user/group IDs and/or the USER statement in your Dockerfiles are different, and the two containers are technically running under different IDs. Different apps will end up running as different IDs. For example, the node base image creates a user called node with ID of 1000, but the NGINX image creates an nginx user as ID 101. Also, some apps spin-off sub-processes as different users. NGINX starts its main process (PID 1) as root (ID 0) but spawns sub-processes as the nginx user (ID 101), which keeps it more secure.

So for troubleshooting, this is what I do:
Use the command ps aux in each container to see a list of processes and usernames. The process needs a matching user ID or group ID to access the files in question.

Find the UID/GID in each containers `/etc/passwd` and `/etc/group` to translate names to numbers. You'll likely find there a miss-match, where one containers process originally wrote the files with its UID/GID and the other containers process is running as a different UID/GID.

Figure out a way to ensure both containers are running with either a matching user ID or group ID. This is often easier to manage in your own custom app (when using a language base image like python or node) rather than trying to change a 3rd party app's container (like nginx or postgres)... but it all depends. This may mean creating a new user in one Dockerfile and setting the startup user with USER. (see USER docs) The node default image has a good example of the commands for creating a user and group with hard-coded IDs:

RUN groupadd --gid 1000 node \\
        && useradd --uid 1000 --gid node --shell /bin/bash --create-home node
USER 1000:1000
Note: When setting a Dockerfile's USER, use numbers, which work better in Kubernetes than using names.

Note 2: If ps doesn't work in your container, you may need to install it. In debian-based images with apt, you can add it with apt-get update && apt-get install procps

### 6.7 Bind Mounts with JekyII

Changes can be *Reflecting* on the container

*Static site generator*: SSG

<?>
`root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/bindmount-sample-1# docker run -p 80:4000 -v $(pwd):/site bretfisher/jekyll-serve`

## 7 Docker Compose

*Relationship between containers*
*Save docker run settings in easy-to-read file*
*YAML-formatted file*
*CLI tool docker-compose* used for local dev/test automation

docker compose file

- has version
Version 2 and above provide significantly more features then the old default version 1, and what we will be using as a default base for this course. Bonus Note: v2.x is actually better for local docker-compose use, and v3.x is better for use in server clusters (Swarm and Kubernetes)
- `.yml` file can be used with `docker-compose` command for local docker automation
- has filename, `docker-compose.yml` is just a default name, use `docker-compose -f` to specify file name
- use service name as DNS name
services, volumes and networks

services下面的服务名称会成为DNS name
The bridge network is created at runtime by default so the containers can communicate with one another across it.

```yml
# version isn't needed as of 2020 for docker compose CLI.
# All 2.x and 3.x features supported
# version: '2'

services:

  wordpress:
    image: wordpress
    ports:
      - 8080:80
    environment:
      WORDPRESS_DB_HOST: mysql
      WORDPRESS_DB_NAME: wordpress
      WORDPRESS_DB_USER: example
      WORDPRESS_DB_PASSWORD: examplePW
    volumes:
      - ./wordpress-data:/var/www/html

  mysql:
    # we use mariadb here for arm support
    # mariadb is a fork of MySQL that's often faster and better multi-platform
    image: mariadb
    environment:
      MYSQL_ROOT_PASSWORD: examplerootPW
      MYSQL_DATABASE: wordpress
      MYSQL_USER: example
      MYSQL_PASSWORD: examplePW
    volumes:
      - mysql-data:/var/lib/mysql

volumes:
  mysql-data:
```

```yml
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/compose-sample-1# cat compose-3.yml
version: '3.9'
# NOTE: This example only works on x86_64 (amd64)
# Percona doesn't yet publish arm64 (Apple Silicon M1) or arm/v7 (Raspberry Pi 32-bit) images

services:
  ghost:
    image: ghost
    ports:
      - "80:2368"
    environment:
      - URL=http://localhost
      - NODE_ENV=production
      - MYSQL_HOST=mysql-primary
      - MYSQL_PASSWORD=mypass
      - MYSQL_DATABASE=ghost
    volumes:
      - ./config.js:/var/lib/ghost/config.js
    depends_on:
      - mysql-primary
      - mysql-secondary
  proxysql:
    # image only works on x86_64 (amd64)
    image: percona/proxysql
    environment:
      - CLUSTER_NAME=mycluster
      - CLUSTER_JOIN=mysql-primary,mysql-secondary
      - MYSQL_ROOT_PASSWORD=mypass

      - MYSQL_PROXY_USER=proxyuser
      - MYSQL_PROXY_PASSWORD=s3cret
  mysql-primary:
    # image only works on x86_64 (amd64)
    image: percona/percona-xtradb-cluster:5.7
    environment:
      - CLUSTER_NAME=mycluster
      - MYSQL_ROOT_PASSWORD=mypass
      - MYSQL_DATABASE=ghost
      - MYSQL_PROXY_USER=proxyuser
      - MYSQL_PROXY_PASSWORD=s3cret
  mysql-secondary:
    # image only works on x86_64 (amd64)
    image: percona/percona-xtradb-cluster:5.7
    environment:
      - CLUSTER_NAME=mycluster
      - MYSQL_ROOT_PASSWORD=mypass

      - CLUSTER_JOIN=mysql-primary
      - MYSQL_PROXY_USER=proxyuser
      - MYSQL_PROXY_PASSWORD=s3cret
    
    # depends_on表明容器之间的依赖关系
    depends_on:
      - mysql-primary
```

2022年docker compose V2问世

### 7.1 Docker Compose Commands

Not a production-grade tool but ideal for local development and test

`docker compose up -d`          后台运行
`docker compose down`
`docker compose down -v`        clean up the containers
`docker compose down --rmi`     remove images

`docker compose logs`
`docker compose ps`
`docker compose top`

反向代理nginx服务器+阿帕奇服务器

```yml
root@IVT-WKS-000223:/mnt/c/Documents and Settings/hzhang3/Documents/udemy-docker-mastery/compose-sample-2# cat docker-compose.yml
# version isn't needed as of 2020 for docker compose CLI.
# All 2.x and 3.x features supported
#version: '3.9'

services:
  proxy:
    image: nginx:1.23 # this will use the latest version of 1.23
    ports:
      # NOTE: if port 80 is already in use on your host, this won't work
      # in that case, change to any high port, like '8000:80'
      # and then use http://localhost:8000 to access the proxy
      - '80:80' # expose 80 on host and sent to 80 in container
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
  web:
    image: httpd:2  # this will use the latest apache v2
```

> When building your Dockerfile and docker-compose.yml file, remember that you'll need to check the compatible versions in that apps documentation.

### 7.2 Assignment1

```yml
# version isn't needed as of 2020 for docker compose CLI. 
# All 2.x and 3.x features supported
# version: '2'

services:
  drupal:
    image: drupal:9
    ports:
      - "8080:80"
    volumes:
      - drupal-modules:/var/www/html/modules
      - drupal-profiles:/var/www/html/profiles       
      - drupal-sites:/var/www/html/sites      
      - drupal-themes:/var/www/html/themes
  postgres:
    image: postgres:14
    environment:
      - POSTGRES_PASSWORD=mypasswd

volumes:
  drupal-modules:
  drupal-profiles:
  drupal-sites:
  drupal-themes:
```

### 7.3 Use Special Docker File

会先检查本地cache中是否保存有名为`nginx-custom`的镜像，如果没有，则会通过`nginx.Dockerfile`这个文件构建

如果你想重新构建，则需要使用`docker compose build`

如果不在yml文件中添加image选项，则docker会默认在image前根据工作路径进行命名，如果想要自动化删除，则需要运行`docker compose down --rmi <local>`指令，这会自动删除全部的本地镜像

```yml
services:
  proxy:
    build:
      context: .
      dockerfile: nginx.Dockerfile
    image: nginx-custom
    ports:
      - '80:80'
  web:
    image: httpd
    volumes:
      - ./html:/usr/local/apache2/htdocs/
```

### 7.4 Assignment2

```yml
# version isn't needed as of 2020 for docker compose CLI. 
# All 2.x and 3.x features supported
# version: '2'

services:
  drupal:
    build:
      context: .
      dockerfile: ./Dockerfile
    image: constum-drupal
    ports:
      - "8080:80"
    volumes:
      - drupal-modules:/var/www/html/modules
      - drupal-profiles:/var/www/html/profiles       
      - drupal-sites:/var/www/html/sites      
      - drupal-themes:/var/www/html/themes
      - drupal-data:/var/lib/postgresql/data
  postgres:
    image: postgres:14
    environment:
      - POSTGRES_PASSWORD=mypasswd
    volumes:
      - drupal-data:/var/lib/postgresql/data

volumes:
  drupal-modules:
  drupal-profiles:
  drupal-sites:
  drupal-themes:
  drupal-data:
  drupal-data:
```
